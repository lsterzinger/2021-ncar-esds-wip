{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCAR Earth System Data Science WIP Talk\n",
    "__This presentation is based on work I did during the [NCAR Summer Internship in Parallel Computational Science (SIParCS) program](https://www2.cisl.ucar.edu/siparcs)__\n",
    "### Lucas Sterzinger -- Atmospheric Science PhD Candidate at UC Davis\n",
    "* [Twitter](https://twitter.com/lucassterzinger)\n",
    "* [GitHub](https://github.com/lsterzinger)\n",
    "* [Website](https://lucassterzinger.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Motivation:\n",
    "* NetCDF is not cloud optimized\n",
    "* Other formats, like Zarr, aim to make accessing and reading data from the cloud fast and painless\n",
    "* However, most geoscience datasets available in the cloud are still in their native NetCDF/HDF5, so a different access method is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do I mean when I say \"Cloud Optimized\"?\n",
    "![Move to cloud diagram](images/cloud-move.png)\n",
    "\n",
    "In traditional scientific workflows, data is archived in a repository and downloaded to a separate computer for analysis (left). However, datasets are becoming much too large to fit on personal computers, and transferring full datasets from an archive to a seperate machine can use lots of bandwidth.\n",
    "\n",
    "In a cloud environment, the data can live in object storage (e.g. AWS S3), and analysis can be done in an adjacent compute instances, allowing for low-latency and high-bandwith access to the dataset.\n",
    "\n",
    "## Why NetCDF doesn't work well in this workflow\n",
    "\n",
    "NetCDF is probably the most common binary data format for atmospheric/earth sciences, and has a lot of official and community support. However, the NetCDF format/API requires either a) loading the entire dataset in order to access the header/metadata and retreive a chunk of data.\n",
    "\n",
    "![NetCDF File Object](images/single_file_object.png)\n",
    "\n",
    "## The Zarr Solution\n",
    "The [Zarr data format](https://zarr.readthedocs.io/en/stable/) alleviates this problem by storing the metadata and chunks in seperate files that can be accessed as-needed and in parallel.\n",
    "\n",
    "![Zarr](images/zarr.png)\n",
    "\n",
    "## _However_\n",
    "While Zarr proves to be very good for this cloud-centric workflow, most cloud-available data is currently only available in NetCDF/HDF5/GRIB2 format. While it would be _wonderful_ if all this data converted to Zarr overnight, in the meantime it would be great if there was a way to use some of the Zarr spec, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducting `fsspec-reference-maker`\n",
    "[Github page](https://github.com/intake/fsspec-reference-maker)\n",
    "\n",
    "`fsspec-reference-maker` works by analysing the NetCDF header/metadata info, extracting byte-ranges for each variable chunk, and creating a Zarr-spec metadata file. This file is plaintext and can opened and analyzed with xarray very quickly. When a user requests a certain chunk of data, the NetCDF4 API is bypassed entirely and the Zarr API is used to extract the specified byte-range.\n",
    "\n",
    "![reference-maker vs zarr](images/referencemaker_v_zarr.png)\n",
    "\n",
    "## How much of a difference does this make, really?\n",
    "Testing this method on 24 hours of 5-minute GOES-16 data and accessing via native NetCDF, Zarr, and NetCDF + ReferenceMaker:\n",
    "\n",
    "![workflow results](images/workflow_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `fsspec-reference-maker` and make sure it's at the latest version (`0.0.3` at the time of writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec_reference_maker\n",
    "fsspec_reference_maker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from fsspec_reference_maker.hdf import SingleHdf5ToZarr\n",
    "from fsspec_reference_maker.combine import MultiZarrToZarr\n",
    "import fsspec\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an S3 filesystem for listing GOES files on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = fs.glob(\"s3://noaa-goes16/ABI-L2-SSTF/2020/210/*/*.nc\")\n",
    "# flist = fs.glob(\"s3://noaa-goes16/ABI-L2-MCMIPC/2021/049/*/*.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepend `s3://` to the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = ['s3://' + f for f in flist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "flist_bag = db.from_sequence(flist, npartitions=len(flist))\n",
    "flist_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definte function to return a reference dictionary for a given S3 file URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ref(f):\n",
    "    so = dict(\n",
    "        mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"none\"\n",
    "    )\n",
    "\n",
    "    with fsspec.open(f, **so) as infile:\n",
    "        return SingleHdf5ToZarr(infile, f, inline_threshold=300).translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map `gen_ref` to each member of `flist_bag` and compute\n",
    "_Note: if running interactively on Binder, this will take a while since only one worker is available and the references will have to be generated in serial. See option for loading from jsons below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dicts = flist_bag.map(gen_ref).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual dictionaries can be saved as JSON files if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "# for d in dicts:\n",
    "#     # Generate name from corresponding URL:\n",
    "#     # Grab URL, strip everything but the filename, \n",
    "#     # and replace .nc with .json\n",
    "#     name = d['templates']['u'].split('/')[-1].replace('.nc', '.json')\n",
    "\n",
    "#     with open(name, 'w') as outf:\n",
    "#         outf.write(ujson.dumps(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generated jsons can then be loaded back in as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "# dicts = []\n",
    "\n",
    "# for f in sorted(glob('./example_jsons/individual/*.json')):\n",
    "#     with open(f,'r') as fin:\n",
    "#         dicts.append(ujson.load(fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `MultiZarrToZarr` to combine the 24 individual references into a single reference\n",
    "In this example we passed a list of reference dictionaries, but you can also give it a list of `.json` filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz = MultiZarrToZarr(\n",
    "    dicts,\n",
    "    # sorted((glob('./example_jsons/individual/*.json'))),\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon':True},\n",
    "    xarray_open_kwargs={\n",
    "        \"decode_cf\" : False,\n",
    "        \"mask_and_scale\" : False,\n",
    "        \"decode_times\" : False,\n",
    "        \"decode_timedelta\" : False,\n",
    "        \"use_cftime\" : False,\n",
    "        \"decode_coords\" : False\n",
    "    },\n",
    "    xarray_concat_args={'dim' : 't'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References can be saved to a file (`combined.json`) or passed back as a dictionary (`mzz_dict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz.translate('./combined.json')\n",
    "# mzz_dict = mzz.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the referenced files with `fsspec` and `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metpy\n",
    "import cartopy.crs as ccrs\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use metpy's `parse_cf()` to generate projection information for future plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs2 = fsspec.filesystem('reference', fo=\"./example_jsons/combined.json\", remote_protocol='s3', remote_options=dict(anon=True), skip_instance_cache=True)\n",
    "fs2 = fsspec.filesystem('reference', fo=\"./combined.json\", remote_protocol='s3', remote_options=dict(anon=True), skip_instance_cache=True)\n",
    "ds = xr.open_dataset(fs2.get_mapper(\"\"), engine='zarr').metpy.parse_cf()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use metpy to calculate lat/lon based on the GOES projection grid, and rename time dimension (for better plotting with hvplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.metpy.assign_latitude_longitude()\n",
    "ds['t'].attrs['long_name'] = 'Time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lat = (ds.latitude > 20) & (ds.latitude < 60)\n",
    "mask_lon = (ds.longitude > -100) & (ds.longitude < -50)\n",
    "\n",
    "\n",
    "sst = ds.SST.where(mask_lat & mask_lon, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst.hvplot.quadmesh(\n",
    "    'x', 'y', groupby='t',\n",
    "    crs = sst.metpy.cartopy_crs, projection=ccrs.Orthographic(-75,30),\n",
    "    features = ['land', 'borders'], width=500,\n",
    "    project=True, rasterize=True, coastline=True, cmap='jet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst.mean(dim='t').hvplot.quadmesh(\n",
    "    'x', 'y',\n",
    "    crs = sst.metpy.cartopy_crs, projection=ccrs.Orthographic(-75,30),\n",
    "    features = ['land', 'borders'], width=500,\n",
    "    project=True, rasterize=True, coastline=True, cmap='jet'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d853cbf2f35f45a59f79ca5e397d8dd1594080251b0b51418fe33f5fb0138a7a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
